# 网课学习-邹博

### 1.概率

##### 	本福特定律:

在实际生活得出的一组数据中，以1为首位数字出现的概率约为总数的1/3，比预想的1/9 大了三倍。

·阶乘/素数数列/斐波那契数列首位

·住宅地址号码

·经济数据反欺诈

·选举投票反欺诈

例子：商品推荐

##### 	数学基础：

###### 		1.二项分布（Bernoulli distribution）：

​		

###### 		2.泊松分布（Taylor展式）：

​			期望和方差都等于λ

###### 		3.均匀分布：

###### 		4.指数分布：

###### 		5.正态分布:



###### <img src="C:\Users\Administrator\Desktop\Typora\图片\概率.png" alt="概率" style="zoom:50%;" />



###### 6.BETA分布 ---> Dirichlet分布

###### 7.负二项分布

sigmoid函数

### 4.Python基础

"提升"

```python
#标准的python支持列表list，但是需要指针和整数对象。

#Numpy提供了ndarray对象，存储单一数据类型的多维数组。
```



#### 1.使用array创建

##### #通过array函数传递list对象

```python
L = [1,2,3,4,5,6]
a = np.array(L)

#若传递的是多层嵌套的list，将创建多维数组
b = np.array([1,2,3],[4,5,6],[7,8,9])
```

##### #数组大小可以通过shape属性获得

```python
#也可以强制修改shape
b.shape = 4,3
#当某个轴为-1时，将根据元素的个数自动计算此轴的长度
b.shape = 2, -1
```

##### #使用reshape方法，可以获得一个新的改变尺寸数组

```python
#数组b和c共享内存，修改任意一个将影响另外一个
c = b.reshape((4,-1))
b[0][1]=20
#c中的[0][1]也会改变
#可以通过dtype参数在创建时指定元素类型
d = np.array([[1,2,3,4],[5,6,7,8]],dtype=np.float)
##如果更改元素类型，可以使用astype安全转换。不要强制修改元素的类型
```



#### 2.使用函数创建

##### #arrange：指定起始值，终止值，步长来创建数组

```python
a = np.arrange(1,10,0.5)
```

##### #linspace函数

```python
#通过指定起始值、终止值，元素个数来创建数组，缺省包括终止值。可以通过endpoint关键字指定是否包括终值

b = np.linspace(1,10,10,endpoint=False)
```

##### #logspace可以创建等比数列

```python
#下面函数创建起始值为10^1,终止值为10^2,有10个数的等比.base默认为10，可以改成为2
d = np.logspace(1,2,10,endpoint=True,base=2)
```

##### #使用frombuffer,fromstring

可以从字节序列创建数组

```python
s = 'abcdzzz'
g = np.fromstring(s,dtype=np.int8)
```



#### 3.存取

##### 3.1切片

```python
a[::-1]
#步长为-1，即翻转。此时刚好取反
#切片数据是原数组的一个视图，内容空间是共享的，可以直接修改元素值
a[1:4]=10,20,30
#在实践中，切实注意原始数据是否被破坏
b = a[2:5]
b[0] = 200
#此时a被修改
#修改切片可以先 c =np.array(b[1:5]) 此时即为新数据
```

##### 3.2 整数/bool数组存取

```
b = a[a>0.5]
a[a>0.5] = 0.5
```

##### 3.3 二维数组的切片

```python
b = a.reshape((-1,1)) #转换成列向量
c = np.arange(6) 
f = b + c # 行+列

a = np.arange(0,60,10).reshape((-1,1))+np.arange(6)
#二维数组的切片
a[[0,1,2],[2,3,4]] #第0行第2列 第1行第3列 第2行第4列
a[4,[2,3,4]]
a[4:,[2,3,4]]
```



#####  	4.1Numpy与python数学库的时间比较

##### 	4.2 元素去重

```python
b = np.unique(a) #去重a
#二维数组的去重，结果是预期的么？
#unique只能处理和返回一维数据

#方案1:转换为虚数
x = c[:,0]+c[:,1]*1j
idx = np.unique(x,return_index=True)[1]
c[idx]

#方案2：利用set
np.array(list(set([tuple(t) for t in c])))
```

##### 	4.3 stack和axis

##### 	4.4 np.dot

​	dot为矩阵乘法



#### 5.绘图

```
mu = 0 #均值
sigma = 1 #方差
x = np.linspace (mu - 3*sigma,mu + 3*sigma, 51)
y = np.exp(-(x-mu)**2/(2*sigma**2))/(math.sqrt)
plt.figure(facecolor='w') #背景为白色
plt.plot(x,y,'r-',x,y,'go',linewidth=2,markersize=8)
#'r-'为实线 'go'为圆圈，markersize为圈的大小
plt.grid(True) #网格
```

##### 5.2 损失函数

Logistic损失(-1,1) / SVM Hinge损失 / 0/1损失

```python
x = np.array(np.linspace(start=-2,stop=3,num=1001,dtype=np.float))
y_logit = np.log(1+np.exp(-x))/math.log(2)
y_boost = np.exp(-x)
y_01 = x<0
y_hinge = 1.0-x
y_hinge[y_hinge<0]=0
plt.plot(x,y_logit,'r-',label='Logistic',linewidth=2)
plt.plot(x,y_01,'g-',label='0/1',linewidth=2)
plt.plot(x,y_linge,'b-',label='Hinge',linewidth=2)
plt.grid()
plt.legend(loc='upper right') #label位置
plt.show()
```

#### 6.概率分布

```python
#6.1均匀分布
x = np.random.rand(10000)
t = np.arange(len(x))
plt.hist(x,30,color='m',alpha=0.8)
plt.legend(loc='upper left')
plt.grid()
plt.show()
```



### 7.回归

##### 1.线性回归

##### 	使用极大似然估计解释最小二乘

​	yi = θxi + εi

其中εi(1<=i<=m)是独立同分布的，服从均值为0，方差为某定值σ²的高斯分布。原因:中心极限定理。

（Andrew Ng）

最小二乘意义下的参数最优解:

​	参数的解析式： θ=（X^T·X)^-1 ·X^T ·y

​	若X^T·X不可逆或者防止过拟合，增加λ扰动：

​	θ=（X^T·X+λ·I)^-1 ·X^T ·y

​	"简便"方法记忆结论：

​	Xθ=y => X^T·Xθ=X^T · y

==> θ=(X^T·X)^-1 ·X^T ·y

假设具有的性质：

1.内涵性：

假设往往是正确的，但不一定总是正确的。

2.简化性：

需要接近真实，往往需要做若干简化。

3.发散性：

在某个简化的假设下推导得到的结论，不一定只有在假设成立时结论才成立。



##### 龙格现象

##### 2.线性回归的复杂度惩罚因子

将目标函数增加平方和损失：(**正则项**)

L2-norm: λΣ（j=1~n）θj²	(用θ代入来控制足够小，同时λ设置参与度)（性能较好）

本质即为假定参数θ服从高斯分布：**Ridge回归**

另外还有:

LASSO（L1-norm）：λΣ（j=1~n）|θj|	(特征选择能力)

Elastic Net (把两者结合起来):

λ(ρΣ(j=1~n)|θj| + (1-ρ)Σ(j=1~n)θj²）

**“稀疏解”**

##### 3.机器学习与数据使用

训练数据->θ  验证数据->λ  测试数据

其中λ为超参数

交叉验证

##### 4.Moore-Penrose广义逆矩阵（伪逆）

当A为矩阵（非方阵）时，称A+为A的广义逆（伪逆）

奇异值分解SVD

​	A=UΣV^T --->  A+ = ····

##### 5.梯度下降算法（最常用）

初始化θ（随机初始化）

沿着负梯度方向迭代，更新后的θ 使J(θ )更小

θ =θ -α（偏导 J(θ) / θ ） 其中α为学习率



随机梯度下降（SGD）（优先选择SGD）

折中：mini-batch SGD (统一也称为SGD)



##### 6.线性回归的进一步分析

可以对样本是非线性的，只要对参数θ线性

***特征选择***



**7.局部加权线性回归**：

权值的设置	w的一种可能的选择方式（RBF）高斯核函数



思考：如何用回归解决分类问题?

不建议直接用回归做分类



### 8.线性回归-Logistic回归

Logistic/sigmoid函数

y = 1/（1+e^-x）



##### Logistic回归参数估计

对比线性回归的结果：

具有相同的形式！只是h(x)各不相同

##### 对数线性模型:

一个事件的几率odds，是指该事件发生的概率与该事件不发生的概率的比值。

对数几率：logit函数

logit(p)=log p/1-p =log h(x)/1-h(x) = θ^T·x

##### Logistic回归的损失：

yi∈{-1，1}

(yi+1)/2 -->  0,1

-(yi-1)/2---> -1,0

L（θ）=》l(θ)

loss(yi,yi^) = -l(θ) 	(NLL)

(yi={-1,1} 

yi^={pi,		yi=1, 

​		1-pi, 	yi=-1})

loss()=Σ[ln(1+e^(-yi·fi))]

##### 分类：Logistic回归

沿似然函数正梯度上升



##### Softmax回归

K分类，第k类的参数为θk，组成二维矩阵θ(k*n)

ln(e^x+e^y) --> max(x,y)  近似相等

不是严格的max 是soft-max(softmax回归)

例题:最大熵模型

总结和思考：

1.特征选择很重要吗，除了人工选择，还可以用其他机器学习方法，如随机森林，PCA, LDA等

2.梯度下降算法是参数优化的重要手段，尤其SGD。

​	适用于在线学习、跳出局部最小值

3.Logistic/Softmax回归是实践中解决分类问题的最重要方法

​	方法简单、容易实现、效果良好、易于解释

​	不只是分类，推荐系统。

思考:Loigistic回归的目标函数，可否用相对熵解释？

​		共线性



### 9.回归实践

##### 	AUC(Area Under Curve)

鸢尾花数据集

​	**差分与取对数**

​	**差分的自回归预测值**

​	**滑动平均值**

#csv是用逗号分隔的格式

```python
#最简单的读取 用pandas
data = pd.read_csv(path)
x = data['TV','Radio','Newspaper']
y = data['Sales']
```

##### Logistic回归

```python
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
clf = LogisticRegression(penalty='l2',C=1)
```

