# 网课学习-邹博

### 1.概率

##### 	本福特定律:

在实际生活得出的一组数据中，以1为首位数字出现的概率约为总数的1/3，比预想的1/9 大了三倍。

·阶乘/素数数列/斐波那契数列首位

·住宅地址号码

·经济数据反欺诈

·选举投票反欺诈

例子：商品推荐

##### 	数学基础：

###### 		1.二项分布（Bernoulli distribution）：

​		

###### 		2.泊松分布（Taylor展式）：

​			期望和方差都等于λ

###### 		3.均匀分布：

###### 		4.指数分布：

###### 		5.正态分布:



###### <img src="C:\Users\Administrator\Desktop\Typora\图片\概率.png" alt="概率" style="zoom:50%;" />



###### 6.BETA分布 ---> Dirichlet分布

###### 7.负二项分布

sigmoid函数

### 4.Python基础

"提升"

```python
#标准的python支持列表list，但是需要指针和整数对象。

#Numpy提供了ndarray对象，存储单一数据类型的多维数组。
```



#### 1.使用array创建

##### #通过array函数传递list对象

```python
L = [1,2,3,4,5,6]
a = np.array(L)

#若传递的是多层嵌套的list，将创建多维数组
b = np.array([1,2,3],[4,5,6],[7,8,9])
```

##### #数组大小可以通过shape属性获得

```python
#也可以强制修改shape
b.shape = 4,3
#当某个轴为-1时，将根据元素的个数自动计算此轴的长度
b.shape = 2, -1
```

##### #使用reshape方法，可以获得一个新的改变尺寸数组

```python
#数组b和c共享内存，修改任意一个将影响另外一个
c = b.reshape((4,-1))
b[0][1]=20
#c中的[0][1]也会改变
#可以通过dtype参数在创建时指定元素类型
d = np.array([[1,2,3,4],[5,6,7,8]],dtype=np.float)
##如果更改元素类型，可以使用astype安全转换。不要强制修改元素的类型
```



#### 2.使用函数创建

##### #arrange：指定起始值，终止值，步长来创建数组

```python
a = np.arrange(1,10,0.5)
```

##### #linspace函数

```python
#通过指定起始值、终止值，元素个数来创建数组，缺省包括终止值。可以通过endpoint关键字指定是否包括终值

b = np.linspace(1,10,10,endpoint=False)
```

##### #logspace可以创建等比数列

```python
#下面函数创建起始值为10^1,终止值为10^2,有10个数的等比.base默认为10，可以改成为2
d = np.logspace(1,2,10,endpoint=True,base=2)
```

##### #使用frombuffer,fromstring

可以从字节序列创建数组

```python
s = 'abcdzzz'
g = np.fromstring(s,dtype=np.int8)
```



#### 3.存取

##### 3.1切片

```python
a[::-1]
#步长为-1，即翻转。此时刚好取反
#切片数据是原数组的一个视图，内容空间是共享的，可以直接修改元素值
a[1:4]=10,20,30
#在实践中，切实注意原始数据是否被破坏
b = a[2:5]
b[0] = 200
#此时a被修改
#修改切片可以先 c =np.array(b[1:5]) 此时即为新数据
```

##### 3.2 整数/bool数组存取

```
b = a[a>0.5]
a[a>0.5] = 0.5
```

##### 3.3 二维数组的切片

```python
b = a.reshape((-1,1)) #转换成列向量
c = np.arange(6) 
f = b + c # 行+列

a = np.arange(0,60,10).reshape((-1,1))+np.arange(6)
#二维数组的切片
a[[0,1,2],[2,3,4]] #第0行第2列 第1行第3列 第2行第4列
a[4,[2,3,4]]
a[4:,[2,3,4]]
```



#####  	4.1Numpy与python数学库的时间比较

##### 	4.2 元素去重

```python
b = np.unique(a) #去重a
#二维数组的去重，结果是预期的么？
#unique只能处理和返回一维数据

#方案1:转换为虚数
x = c[:,0]+c[:,1]*1j
idx = np.unique(x,return_index=True)[1]
c[idx]

#方案2：利用set
np.array(list(set([tuple(t) for t in c])))
```

##### 	4.3 stack和axis

##### 	4.4 np.dot

​	dot为矩阵乘法



#### 5.绘图

```
mu = 0 #均值
sigma = 1 #方差
x = np.linspace (mu - 3*sigma,mu + 3*sigma, 51)
y = np.exp(-(x-mu)**2/(2*sigma**2))/(math.sqrt)
plt.figure(facecolor='w') #背景为白色
plt.plot(x,y,'r-',x,y,'go',linewidth=2,markersize=8)
#'r-'为实线 'go'为圆圈，markersize为圈的大小
plt.grid(True) #网格
```

##### 5.2 损失函数

Logistic损失(-1,1) / SVM Hinge损失 / 0/1损失

```python
x = np.array(np.linspace(start=-2,stop=3,num=1001,dtype=np.float))
y_logit = np.log(1+np.exp(-x))/math.log(2)
y_boost = np.exp(-x)
y_01 = x<0
y_hinge = 1.0-x
y_hinge[y_hinge<0]=0
plt.plot(x,y_logit,'r-',label='Logistic',linewidth=2)
plt.plot(x,y_01,'g-',label='0/1',linewidth=2)
plt.plot(x,y_linge,'b-',label='Hinge',linewidth=2)
plt.grid()
plt.legend(loc='upper right') #label位置
plt.show()
```

#### 6.概率分布

```python
#6.1均匀分布
x = np.random.rand(10000)
t = np.arange(len(x))
plt.hist(x,30,color='m',alpha=0.8)
plt.legend(loc='upper left')
plt.grid()
plt.show()
```



### 7.回归

##### 1.线性回归

##### 	使用极大似然估计解释最小二乘

​	yi = θxi + εi

其中εi(1<=i<=m)是独立同分布的，服从均值为0，方差为某定值σ²的高斯分布。原因:中心极限定理。

**（Andrew Ng）**

最小二乘意义下的参数最优解:

​	参数的解析式： θ=（X^T·X)^-1 ·X^T ·y

​	若X^T·X不可逆或者防止过拟合，增加λ扰动：

​	θ=（X^T·X+λ·I)^-1 ·X^T ·y

​	"简便"方法记忆结论：

​	Xθ=y => X^T·Xθ=X^T · y

==> θ=(X^T·X)^-1 ·X^T ·y

假设具有的性质：

1.内涵性：

假设往往是正确的，但不一定总是正确的。

2.简化性：

需要接近真实，往往需要做若干简化。

3.发散性：

在某个简化的假设下推导得到的结论，不一定只有在假设成立时结论才成立。



##### 龙格现象

##### 2.线性回归的复杂度惩罚因子

将目标函数增加平方和损失：(**正则项**)

L2-norm: λΣ（j=1~n）θj²	(用θ代入来控制足够小，同时λ设置参与度)（性能较好）

本质即为假定参数θ服从高斯分布：**Ridge回归**

另外还有:

LASSO（L1-norm）：λΣ（j=1~n）|θj|	(特征选择能力)

Elastic Net (把两者结合起来):

λ(ρΣ(j=1~n)|θj| + (1-ρ)Σ(j=1~n)θj²）

**“稀疏解”**

##### 3.机器学习与数据使用

训练数据->θ  验证数据->λ  测试数据

其中λ为超参数

交叉验证

##### 4.Moore-Penrose广义逆矩阵（伪逆）

当A为矩阵（非方阵）时，称A+为A的广义逆（伪逆）

奇异值分解SVD

​	A=UΣV^T --->  A+ = ····

##### 5.梯度下降算法（最常用）

初始化θ（随机初始化）

沿着负梯度方向迭代，更新后的θ 使J(θ )更小

θ =θ -α（偏导 J(θ) / θ ） 其中α为学习率



随机梯度下降（SGD）（优先选择SGD）

折中：mini-batch SGD (统一也称为SGD)



##### 6.线性回归的进一步分析

可以对样本是非线性的，只要对参数θ线性

***特征选择***



**7.局部加权线性回归**：

权值的设置	w的一种可能的选择方式（RBF）高斯核函数



思考：如何用回归解决分类问题?

不建议直接用回归做分类



### 7.线性回归-Logistic回归

Logistic/sigmoid函数

y = 1/（1+e^-x）



##### Logistic回归参数估计

对比线性回归的结果：

具有相同的形式！只是h(x)各不相同

##### 对数线性模型:

一个事件的几率odds，是指该事件发生的概率与该事件不发生的概率的比值。

对数几率：logit函数

logit(p)=log p/1-p =log h(x)/1-h(x) = θ^T·x

##### Logistic回归的损失：

yi∈{-1，1}

(yi+1)/2 -->  0,1

-(yi-1)/2---> -1,0

L（θ）=》l(θ)

loss(yi,yi^) = -l(θ) 	(NLL)

(yi={-1,1} 

yi^={pi,		yi=1, 

​		1-pi, 	yi=-1})

loss()=Σ[ln(1+e^(-yi·fi))]

##### 分类：Logistic回归

沿似然函数正梯度上升



##### Softmax回归

K分类，第k类的参数为θk，组成二维矩阵θ(k*n)

ln(e^x+e^y) --> max(x,y)  近似相等

不是严格的max 是soft-max(softmax回归)

例题:最大熵模型

总结和思考：

1.特征选择很重要吗，除了人工选择，还可以用其他机器学习方法，如随机森林，PCA, LDA等

2.梯度下降算法是参数优化的重要手段，尤其SGD。

​	适用于在线学习、跳出局部最小值

3.Logistic/Softmax回归是实践中解决分类问题的最重要方法

​	方法简单、容易实现、效果良好、易于解释

​	不只是分类，推荐系统。

思考:Loigistic回归的目标函数，可否用相对熵解释？

​		共线性



### 8.回归实践

##### 	AUC(Area Under Curve)

鸢尾花数据集

​	**差分与取对数**

​	**差分的自回归预测值**

​	**滑动平均值**

#csv是用逗号分隔的格式

```python
#最简单的读取 用pandas
data = pd.read_csv(path)
x = data['TV','Radio','Newspaper']
y = data['Sales']
```

##### Logistic回归

```python
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
clf = LogisticRegression(penalty='l2',C=1)
```



##2020/9/22

### 9.决策树和随机森林



##### 信息熵

P(x,y)=P(x)*P(y)

想转化为加法方便计算：ln P(x,y) = ln P(x) + ln P(y)

同时ln P(x)为增函数，但是需要一个减函数

->添加一个负号： -lnP(x)

**条件熵：H(Y|X)**

​	H(X,Y)-H(X)

**相对熵**（KL散度）

D(p||q) = Σx p(x)log p(x)/q(x)

 可以度量两个随机变量的“距离”

**互信息**：

​	I(X，Y)= D（P(X,Y)|| P(X)P(Y) ）

weka数据集

决策树学习采用的是自顶向下的递归方法，其基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处的熵值为零，此时每个叶节点中的实例都属于同一类。

##### 决策树学习的生成算法

**ID3**

使用信息增益g(D,A)进行特征选择

**C4.5**

信息增益率：gr(D,A) = g / H(A) (过滤本身熵过大的样本)

**CART**

基尼系数

##### 关于Gini系数的讨论(一家之言)

​	-lnx --> -x+1 (lnx在x=1处斜率为1，过点(1,0)，斜线即为1-x)

用直线来近似曲线，得到误差足够小，可以代替来评估。

将-lnx在x=1处泰勒展开，忽略高阶无穷小。

##### Gini系数的第二定义

##### 决策树的评价

C(T) = ΣNt · H(t)	 (t∈leaf)

也可以视作损失函数，C越小越好。

##### 决策树的过拟合

剪枝

随机森林

Bootstraping

##### Bagging的策略

从样本集中重采样(有重复的)选出n个样本

在所有属性上，对这n个样本建立分类器。

（**ID3,C4.5,CART,**SVM,Logistic回归等）（用弱分类器，而避免使用强分类器，避免一些英雄样本使得决策不会发生变化）

OOB数据：Bootstrap每次约有36.8%的样本不会出现在采集的样本集合中，将没有参与训练的数据称为袋外数据(OOB Out Of Bag)。它可以用于取代测试集用于误差估计。得到的模型参数是无偏估计。

##### 随机森林

投票机制

##### 样本不均衡的常用处理方法

假定样本数目A类比B类多，且严重不平衡：

​	**A类欠采样  Undersampling**

​		随机欠采样

​		A类分成若干子类，分别与B类进入ML模型

​		基于聚类的A类分割

​	B类过采样Oversampling

​		避免欠采样造成的信息丢失

​	B类数据合成Synthetic Data Generation

​		随机插值得到新样本

​		SMOTE

使用RF建立计算样本间相似度

使用随机森林计算特征重要度

### 10.决策树和随机森林实践

基本参数：

```python
from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassfier
#具体分类参数如下：
DecisionTreeClassifier(criterion='gini',max_depth=None,min_samples_split=2,min_samples_split=2,min_samples_leaf=1,max_leaf_nodes=None)
#1、 criterion: 特征选取方法，可以是gini（基尼系数），entropy（信息增益），通常选择gini，即CART算法，如果选择后者，则是ID3和C4,.5
#2、 max_depth: 树的最大深度，默认可以不输入，那么不会限制子树的深度，一般在样本少特征也少的情况下，可以不做限制，但是样本过多或者特征过多的情况下，可以设定一个上限，一般取10~100
#3、 min_samples_split:节点再划分所需最少样本数，如果节点上的样本树已经低于这个值，则不会再寻找最优的划分点进行划分，且以结点作为叶子节点，默认是2，如果样本过多的情况下，可以设定一个阈值，具体可根据业务需求和数据量来定
#4、 min_samples_leaf: 叶子节点所需最少样本数，如果达不到这个阈值，则同一父节点的所有叶子节点均被剪枝，这是一个防止过拟合的参数，可以输入一个具体的值，或小于1的数（会根据样本量计算百分比）
#5、 min_weight_fraction_leaf: 叶子节点所有样本权重和，如果低于阈值，则会和兄弟节点一起被剪枝，默认是0，就是不考虑权重问题。这个一般在样本类别偏差较大或有较多缺失值的情况下会考虑
#6、min_impurity_decrease 划分最需最小不纯度，特征选择时低于就不考虑这个特征.

#回归参数
DecisionTreeRegressor(criterion='mse',min_samples_split=2,min_samples_split=2,min_samples_leaf=1)
#基本一致，主要在于criterion
#mse或mae，前者是均方差，后者是和均值的差的绝对值之和，一般用前者，因为前者通常更为精准，且方便计算
```

必要库：

```python
# 导入库
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
import pydotplus
from sklearn import tree
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV
```

