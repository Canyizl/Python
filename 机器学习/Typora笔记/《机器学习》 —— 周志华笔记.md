#  《机器学习》 —— 周志华

### 1.绪论

预测的是离散值，”好瓜和坏瓜“，称为**”分类**“；

预测的是连续值，”西瓜成熟度“，称为**"回归"**；

我们还可以对西瓜做**"聚类"**。将训练集中的西瓜分成若干组，每组称为一个“簇”，自动形成的簇可能存在一些潜在的概念划分，如"浅色""深色"。

需要说明的是：在聚类学习中，“浅色瓜”这样的概念我们事先是不知道的，而且学习过程中使用的训练样本通常不拥有标记信息。



根据训练数据是否拥有标记信息，学习任务可大致划分为两大类：

**”监督学习“（Supervised learning）**和**”无监督学习“(Unsupervised learning)**

其中”分类“和”回归“属于前者，

”聚类”则属于后者。



机器学习的目标是使学得的模型能很好地适用于“新样本”，而适用于新样本的能力，就叫做"泛化"。



**归纳（induction）**与**演绎（deduction）**是科学推理的两大手段:

归纳是从特殊到一般的"泛化"。

演绎是从一般到特殊的"特化"。

"从样例中学习"显然是一个归纳的过程，因此称为“归纳学习”。



**“归纳学习”**有狭义和广义之分：

广义的归纳学习大体相当于从样例中学习，

而狭义的归纳学习则要求从训练数据中学得概念（concept）,因此称为“概念学习”/“概念形成”。

**概念学习**目前研究和应用都比较少，因为要学得泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生“黑箱”模型。然而对概念学习有所了解，有助于理解机器学习的一些基础思想。

最基本的概念学习是Bool概念学习，即“是”“不是”。



### 2.模型评估与选择

总数据集：训练集+验证集+测试集

经验误差：在训练集上的误差

泛化误差：在”未来“样本上的误差

#### 2.2评估方法

##### 2.2.1留出法：

​	常见做法是大约2/3~4/5的样本用于训练，剩余用于测试



##### 2.2.2交叉验证法（k折交叉验证）：

​	将D划分为k个大小相似的互斥子集，每次用k-1个作为训练集，余下的那个作为测试集；这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回k个测试结果的均值。

​	k最常用取值为10，其他常用k值有5、20等。



##### 2.2.3自助法：

​	给定包含m个样本的数据集D，每次随机选中一个样本拷贝进入D‘，再将样本放回初始数据集D，使得下次仍有可能将其选中，将这个过程重复m次，就得到了包含m个样本的数据集D’。

​	显然D中有一部分样本会在D‘中多次出现，而另一部分样本不会。

在m次采样中始终不被采到的概率是（1-1/m）^m.极限为1/e=0.368.

​	于是我们可以将D’用作训练集，D\D'用作测试集，这样实际评估和期望评估的模型都有m个训练样本，而还有总量约1/3的、没在训练集中出现的样本用于测试。

​	常用于数据集较小、难以有效划分的训练/测试集。当数据量足够的时候，改变了初始的分布会引入估计偏差，因此用留出法和交叉验证法更常用。



#### 2.3性能度量

##### 	2.3.1 错误率与精度

​	错误率：分类错误的样本数占样本总数的比例

​	精度: 分类正确的样本数占样本总数的比例

##### 	2.3.2 查准率、查全率与F1

![分类结果混淆矩阵](C:\Users\Administrator\Desktop\Typora\图片\分类结果混淆矩阵.png)

查准率P：TP/TP+FP 真正例/预测总正例

查全率R:	TP/TP+FN 真正例/总正例

”平衡点“（BEP）：”查准率=查全率“ 衡量了学习器的优越

更常用的是F1度量: ![F1](C:\Users\Administrator\Desktop\Typora\图片\F1.png)

​		F1 = 2·P·R/(P+R) = 2·TP / 样例总数+TP-TN

通过对P和R的重视度不同，F1度量的一般形式——Fp![Fp](C:\Users\Administrator\Desktop\Typora\图片\Fp.png)

其中β>0，度量了查全率对查准率的相对重要性；

β=1时退化为标准的F1；

β>1时，查全率有更大影响；

β<1时，查准率有更大影响；



#### 6.SVM支持向量机

线性可分问题-->基础SVM

线性不可分-->维度爆炸--->对偶问题--->核函数

噪音问题--->软间隔



#### 基础知识补充：

##### 1.zero-mean normalization

最常见的标准化方法就是Z标准化，也是SPSS中最为常用的标准化方法，spss默认的标准化方法就是z-score标准化。

x* = (x - μ ) / σ 

μ为所有样本数据的均值，σ为所有样本数据的标准差。

Python实现:

```python
def shujuguiyi(a,b,M):
    j = 0
    guiyi = np.empty([a,b])            #empty函数
    for j in range(b):
        guiyi[:,j] = (M[:,j] - np.mean(M[:,j])) / np.std(M[:,j])   #数据归一化标准差法
    return guiyi
#定义数据归一函数
```

注意：该种标准化方式要求原始数据的分布可以近似为高斯分布，否则效果会变得很糟糕。适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。

##### 2.损失函数

损失函数：求解最佳参数，需要一个标准来对结果进行衡量，为此我们需要定量化一个目标函数式，使得计算机可以在求解过程中不断地优化。

如线性回归中：L = 1/n Σ(y-(wx+b))**2 （欧拉距离）