# 机器学习





## 1.课程推荐

[浙江大学-研究生机器学习课程](https://www.bilibili.com/video/BV1dJ411B7gh?t=702&p=38)

[邹博](https://www.bilibili.com/video/BV1Tb411H7uC)



## 2.方向介绍

​	机器学习：分类、推荐系统（实战基本会用Opencv）

​	深度学习：NLP、CV









### 感知器算法（Perceptron Algorithm)

第一个机器学习算法：

1.随机选择W和b

2.取一个训练样本（X,y)

​		(i) 若$W^TX + b > 0 \ 且 y = -1, 则：\\W=W-X,b=b-1$

​		(ii) 若$W^TX+b<0 \ 且y = +1,则：\\ W=W+X,b=b+1$

3.再取一个(X,y)，回到(2)

4.终止条件: 直到所有输入输出对 都不满足2.中(i)(ii)之一，退出循环

（线性可分、线性不可分概念当时还未提出，人工智能引来第一次寒冬）



### 多层神经网络（Multiple Layer Neural Network)

​		引入非线性的激活函数，使得拥有非线性

​		后向传播(back propogation)：怎么只从数据，求出神经网络所有的参数，最优暂时还是未知的，但是有许多常用算法



### 梯度下降法求局部极值（Gradient Descent Method）

1.找一个  $wo$

2.设 $k = 0$ ，假设 $\frac{df(w)}{dw}|_{wk}=0$ , 退出循环

​	否则：$W_{k+1} = W_{k} - α *\frac{df(w)}{dw}|_{wk}$ 

Taylor Explanation:
$$
Taylor ： f(w+Δw) = f(w) + \frac{df(w)}{dw}|_{w}*Δw + o(Δw)\\
将我们的条件代入：\\
f(w_{k+1}) = f(w_k) + (\frac{df(w)}{dw}|_{wk})*(-α\frac{df(w)}{dw}|_{wk})+o(Δw)\\
 = f(w_k) - α[\frac{df(w)}{dw}|_{wk}]^2 + o(Δw)\\
 α>0 \ and \ [\frac{df(w)}{dw}|_{wk}]^2>0\\
 f(w_{k+1})<=f(w_k)
$$



BP算法过程：

​	1.随机取(W,b)

​	2.训练样本(X,y),带入网络,可求出所有的(Z,a,y)（前向传播)

​	3.链式法则求偏导(定义一个损失函数E)

​	4.更新：$W^(新)=W^(旧)-α*偏导\\b^(新) = b^(旧)-α*偏导$



### 自动编码机(Auto-encoder)

解决参数(W,b)初始化问题

步骤1：输入X，输出X，固定第一层参数不动，用BP更新权值，训练好第一层后再训练第二层

步骤M: 以此类推，训练好M-1层后，接着训练第M层

最后一步: 用BP对网络进行微调

（特征提取器）





### 信息理论

事件概率$P->0 \ 时\ Info->∞ \\ P->1 \ 时为废话$

$P(X,Y) = P(X) * P(Y) \\ H(X,Y) = H(X) + H(Y) \\ ln(P(X,Y))=lnP(X)+lnP(y)$

$E[lnP] = -(1-p)*ln(1-p)-p*ln(p)\\=-\sum_{i=1}^np_iln(p_i)$

条件熵  $H(X,Y)-H(X)\\H(Y|X) = -\sum_{x,y}p(x,y)\ log \ p(y|x)$

相对熵(KL散度)



### 概率分类法

基本问题：$假设有两个w_1,w_2,某样本X要么∈w_1,要么∈w_2\\求P(w_1|X)与P(w_2|X)$

$其中\ P(w_1|X)+P(w_2|X)=1 \\ 分类问题：若P(w_1|X)>P(w_2|X),则X∈w_1\\若P(w_1|X)<P(w_2|X),则X∈w_2$



Bayes:

$P(w_1|X)= \frac{P(X,w_1)}{P(X)} = \frac{P(X|w_1)*P（w_1)}{P(x)}$

若$P(X|w_1)*P（w_1)>P(X|w_2)*P（w_2)，则X∈w_1$

$P(w_1)P(w_2)叫做w的先验概率$

$P(X|w_1)P(X|w_2)叫做X在w上的条件概率$

$P(X|w_1)P(X|w_2)叫做X在w上的后验概率$