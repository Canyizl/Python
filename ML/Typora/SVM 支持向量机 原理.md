# SVM 支持向量机

SVM是最大化间隔的分类算法

### 1.线性模型

线性可分（Linear Separable）

非线性可分（Non-linear Separable）

  Q1: 哪一条直线是最好的？

​	对误差的容忍程度最高

  d:间隔（margin）

将平行线插到的向量叫支持向量（Support vectors）



#### 定义:

##### 1.训练数据以及标签(x1,y1)·····（xn,yn） 

！！！注意:每个x都是一个向量，y是标签，yi=+1或者-1。

##### 2.线性模型：（w,b） w^T x +b=0 （超平面）（Hyperplane）

w也是一个向量，（和x同维），b是一个常数。

w^T · x 也是一个常数（列X行后是一个值)。

##### 3.一个训练集线性可分是指：

{（xi,yi）}i=1~N； 存在（w,b），使：对于任意的i=1~N，有：

(I) 若yi=+1,则W^T· Xi + b >=0

(II)若yi=-1,则W^T· Xi + b < 0

​	yi [W^T· Xi + b ] >=0



#### 优化问题（Minimize)(凸优化/二次规划)：

最小化 ： 1/2 ||w||^2 = 1/2（w1^2+w2^2+····+wm^2）

​	限制条件（Subject to）：yi[W^T·Xi+b]>=1 (i=1~N)

（1/2 只是为了求导更好化简）

##### 	事实1:W^T· X+b=0 与aW^T· X+ab=0是同一个平面（a是任意正实数）

若（w,b）满足公式1，则（aw,ab）也满足公式1.

##### 	事实2:点到平面距离公式。

平面：w1x+w2y+b=0.	则点（xo,yo）到此平面的距离:

d = |w1xo+w2yo+b| / √ w1^2+w2^2

向量xo到超平面W^T·X+b=0的距离

d = |W^T·Xo + b| / ||w||

​	 (||w||--> √w1^2+····+wm^2，高维下的情况)



我们可以用a去缩放（w,b）-->(aw,ab)

最终使在支持向量xo上有：|W^T·Xo + b| = 1

d = 1/ ||w||



##### Plus: 二次规划（Quadratic Programming）

​	1.目标函数（Objective Function）是二次项

​	2.限制条件为一次项

要么无解， 要么只有一个极值。



### 2.SVM处理非线性

##### 1.

​	最小化: 1/2 ||w||^2 + C(i=1->N) ∑εi (εi 松弛变量 slack variable)

​	限制条件： 1. yi[W^T·Xi+b ]>=1-εi

​						2.εi>=0

C(i=1->N) ∑εi  ---> 正则项（Regulation Term）其中C是一个事先设定的参数

##### 2.高维映射φ(x)

​		x ----> φ(x)

​		低维	高维

例如: x=[a  b] ------>  φ(x)= [a^2 b^2 a b ab]



我们可以不知道无限维映射φ(x)的显示表达

我们只要只得到一个核函数（Kernel Function）

​	K(X1,X2) = φ(x1)^T · φ(x2) 	(φ(x1)与φ(x2)的内积)

则1这个优化式仍然可解。



##### 核函数![核函数](C:\Users\Administrator\Desktop\Typora\图片\核函数.png)

1. K(x1,x2) = exp(-||x1-x2||^2 / 2σ^2)   (高斯核) 
2. K(x1,x2) = (x1^T·x2+1)^d (多项式核)



##### K(x1,x2)能写成φ(x1)^T · φ(x2)的充要条件（Mercer's Theorem）

 1. K(x1,x2) = K(x2,x1) (交换性)

 2. 任意的Ci, Xi(i=1~N),有： （半正定性）

     ∑(i:1->N)∑(j:1->N) Ci·Cj·K(xi,xj) >=0

      

### 3.优化理论

​	《Convex optimization》、《Nonlinear Programming》

##### 原问题(Prime Problem)

​	最小化: f(w)

​	限制条件： gi(w)<=0 (i=1~K)

​						hi(w)=0	(i=1~M)

##### 对偶问题(Dual Problem)

​	定义：L(w,α,β)=f(w)+∑(i=1->k)αi·gi(w)+∑(i=1->M)βi·hi(w)

​							=f(w) + α^T·g(w) + β^T·h(w)

​	最大化：θ(α,β) = inf（所有w）{L(w,α,β)} 	(inf求最小值，在确定α,β情况下遍历w，求出最小值)

​	限制条件：αi>=0



##### 定理:如果w是原问题的解，而α,β是对偶问题的解，则有：

f(w)>=θ(α,β)

证明:θ(α,β)=inf{L(w,α,β)}<=L(w,α,β)=f(w)+∑(i=1->k)αi·gi(w)+∑(i=1->M)βi·hi(w) <= f(w)



##### 定义： G=f(w)-θ(α,β)>=0

G叫做原问题与对偶问题的间距（Duality Gap)

（对于某些特定优化问题，可以证明G=0）



##### 强对偶定理：

若f(w)为凸函数，且g(w)=Aw+b（线性函数），h(w)=CW+d,

则此优化问题的原问题与对偶问题间距为0，即

​	f(w)=θ(α,β)

推出KKT条件：

​	对于任意的i=1~k,

​	或者αi=0,或者gi(w)=0。



##### plus凸函数：

​	w1,w2，λ[0,1]

f(λw1+(1-λ)w2) <= λf(w1) + (1-λ)f(w2)



### 4.原问题转化为对偶问题

​	最小化: 1/2||w||^2  - C(i=1->N) ∑εi (凸函数)

​	限制条件： 1. 1+εi -yi·W^T·Xi-yi·b <=0 （移项）

​						2. εi<=0



对偶问题：

最大化：θ(α,β) = inf（所有w，εi，b）{1/2||w||^2  - C(i=1->N) ∑εi+∑(i=1->N)βi·εi+∑(i=1->N)αi·[1+εi -yi·W^T·Xi-yi·b]}

限制条件：

​	αi>=0

​	βi>=0

​	

有：	Lw=0

​			Lεi=0

​			Lb=0

Lw =>  w=∑(i=1->N)αi·yi·φ(xi)

Lεi => βi+αi=C

Lb =>  ∑(i=1->N)αi·yi=0



1/2||w||^2 = 1/2 W^T·W

​						=1/2 αi·αj·yi·yj·φ(xi)^T·φ(xj)

​						其中最后两项= K(xi,xj)



最大化:θ(α) =∑(i=1->N)αi- 1/2 ∑(i=1->N)∑(j=1->N)αi·αj·yi·yj·K(xi,xj)

限制条件：

​	1.0<=αi<=c  (αi<=c-βi 且βi>=0)

​	2.∑(i=1->N)αi·yi=0

（SMO算法）



测试流程:

​	测试样本X，

​	若W^T·φ(x)+b>=0,则y=+1

​	若W^T·φ(x)+b<0,则y=-1

W^T·φ(x)=∑(i=1->N)αi·yi·K(xi,x)

由KKT：

取一个0<αi<c => βi=c-αi>0

此时βi≠0=》 εi=0

αi≠0 = 》 b= (1-yi·∑(j=1->N)αj·yj·K(xi,xj) ) / yi



### 总结SVM算法:

##### 1.训练流程：

输入{(xi,yi)}i=1~N  (解优化问题)

最大化：θ(α) =∑(i=1~N)αi- 1/2 ∑(i=1->N)∑(j=1~N)αi·αj·yi·yj·K(xi,xj)

限制条件：1. 0<=αi<=c		(SMO算法)

​					2.∑(i=1->N)αi·yi=0



##### 2.算b

找一个0<αi<c,

b= (1-yi·∑(j=1->N)αj·yj·K(xi,xj) ) / yi



##### 3.测试流程

​	输入测试样本X

若∑(i=1->N)αi·yi·K(xi,x)+b>=0,则y=+1

若∑(i=1->N)αi·yi·K(xi,x)+b<0,则y=-1



注意样本的归一化：求出每个维度的均值和方差，在训练和测试样本上同时归一化:

newX = (X-均值) / 标准差
